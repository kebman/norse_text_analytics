# 1112

## S01-01 Neo4j Scaffolding (Low)

```
We are adding Neo4j graph support to this repo.

Tasks:

1) Create file: nta/graph/schema.cypher
   - Add uniqueness constraints for:
     Work.work_id
     Edition.edition_id
     Segment.segment_id
     Token.token_id
     Form.form_id
   - Add indexes for:
     Token.surface
     Form.orthography

2) Create requirements.txt if it does not exist.
   - Add neo4j>=5.0

3) Create nta/graph/db.py:
   - Define Neo4jConfig dataclass (uri, user, password)
   - Implement get_driver()
   - Implement apply_schema() that reads schema.cypher and executes each statement.

4) Create scripts/apply_schema.py
   - Connect using Neo4jConfig
   - Call apply_schema
   - Print confirmation

Make code clean and minimal.
No overengineering.
```

> Ran 

---

## S01-02 Ingest Hávamál JSON into graph (Medium)

```
Add script: scripts/ingest_havamal_json.py

Goal:
Read data/Hávamál1.json (or detect exact filename in /data automatically).

Create graph nodes:

- Work (work_id="havamal")
- Edition (edition_id="havamal_json_v1")
- Segment (one per verse/line depending on JSON structure)
- Token (one per word occurrence)
- Form (unique per surface string per language)

Relationships:
Work -> HAS_EDITION -> Edition
Edition -> HAS_SEGMENT -> Segment
Segment -> HAS_TOKEN -> Token
Token -> INSTANCE_OF_FORM -> Form

Requirements:
- Use MERGE everywhere (idempotent reruns)
- Basic tokenizer (split words, strip punctuation)
- Do NOT implement Lemma yet
- Add clear comments
- Keep it simple and robust

Do not change existing files outside of adding this script.
```

> Ran 

---

## S01-03 Refactor existing bin scripts to use graph instead of JSON (Medium)

```
Refactor bin/numWordsHávamál.py.

Instead of counting words from JSON,
query Neo4j and count Token nodes grouped by surface.

Use the Neo4j Python driver.
Print top 20 most frequent Token.surface values.

Do not delete old logic yet.
Add graph-based version alongside it.
```

> Ran 

---

## S01-04 Add Lemma layer (Medium)

```
Extend the graph model to support Lemma nodes.

Add:
- Lemma node (lemma_id, headword, language, pos)
- Form -> REALIZES -> Lemma relationship

Modify ingest script so:
- For now, Lemma is identical to Form (temporary 1:1 mapping)
- Create Lemma node for each Form if not exists
- Link Form -> REALIZES -> Lemma

We will improve lemmatization later.
Keep architecture clean.
```

> Ran 

---

## S01-05 Make DB credentials environment-driven (Low)

```
Refactor Neo4jConfig to read:
NEO4J_URI
NEO4J_USER
NEO4J_PASSWORD

from environment variables with sensible defaults.

Do not hardcode passwords in source.
```

> Ran 

---

## S01-06 Write canonical schema docs + invariants (Low)

```
Create docs/schema.md documenting the graph data model and invariants.

Include:
- Node types: Work, Witness, Edition, Segment, Token, Form, Lemma, Sense (optional), MorphAnalysis, Etymon, CognateSet, Claim, Source
- Relationship types and intended direction
- Cardinalities/invariants (e.g., Segment HAS_TOKEN Token; Token INSTANCE_OF_FORM Form; Form REALIZES Lemma; Claim SUPPORTED_BY Source)
- Why Token/Form/Lemma are distinct (text evidence vs spelling vs dictionary entry)
- How uncertainty is represented using Claim nodes (allow competing etymologies)
- ID conventions for every node type (stable, deterministic, rerunnable)
- Minimal MVP subset vs full model

Write it as a reference doc for future development. No filler.
```

> Ran 

---

## S01-07 Upgrade schema.cypher to “real system” (Low)

```
Expand nta/graph/schema.cypher to include constraints and indexes for the serious model.

Add uniqueness constraints for:
- Work.work_id
- Witness.witness_id
- Edition.edition_id
- Segment.segment_id
- Token.token_id
- Form.form_id
- Lemma.lemma_id
- Sense.sense_id
- MorphAnalysis.analysis_id
- Etymon.etymon_id
- CognateSet.set_id
- Claim.claim_id
- Source.source_id

Add indexes for query-heavy fields:
- Token.surface
- Token.normalized
- Form.orthography
- Lemma.headword
- Segment.ref (or segment_ref)
- Source.citekey
- Claim.type

Keep Cypher compatible with Neo4j 5.
Do not remove existing constraints; extend them.
```

> Ran 

---

## S01-08 Introduce a proper internal library with typed models (High)

```
Create a minimal internal Python library under nta/ with typed dataclasses and clear boundaries.

Add:
- nta/model/ids.py: functions to generate deterministic IDs for Work/Edition/Segment/Token/Form/etc.
- nta/model/types.py: dataclasses for Work, Edition, Segment, Token, Form, Lemma, Claim, Source (only the core fields)
- nta/graph/repo.py: a Neo4jRepository class with methods:
  - apply_schema()
  - upsert_work()
  - upsert_edition()
  - upsert_segment()
  - upsert_token_and_form()
  - upsert_lemma()
  - upsert_claim()
  - link_* helpers

Requirements:
- Deterministic IDs so reruns are idempotent
- MERGE-based upserts
- No business logic inside repo beyond persistence
- Keep it small; only what ingest needs

Do not rewrite ingestion yet; just add the library.
```

> Ran 

---

## S01-09 Make ingestion “pipeline-based” and stable IDs (High)

```
Refactor scripts/ingest_havamal_json.py into a pipeline using nta/model and nta/graph/repo.

Goals:
- Use stable IDs for Segment and Token based on structured refs:
  Segment: <edition_id>:<verse>:<strophe>:<line> OR a best-effort equivalent based on the JSON structure.
  Token: <segment_id>:<token_index>
- Store:
  Segment.ref (human ref)
  Segment.text (raw line)
  Token.surface (raw)
  Token.normalized (basic normalization; see below)
  Token.position (index)

Normalization v0:
- lowercasing optional (config flag)
- strip surrounding punctuation
- preserve diacritics by default

Implement a small normalization function and store normalized.

Keep script runnable and idempotent.
```

> Ran 

---

## S01-10 Add variant system (Medium)

```
Implement orthographic/normalization variant tracking.

Add:
- Relationship: (Form)-[:ORTHOGRAPHIC_VARIANT_OF {type}]->(Form)
- Relationship: (Token)-[:NORMALIZED_TO {policy}]->(Form) (optional)
- Ingest change:
  - Create a Form for surface and (optionally) a Form for normalized
  - Link surface_form ORTHOGRAPHIC_VARIANT_OF normalized_form with type="punct_strip" (or similar)
  - Token INSTANCE_OF_FORM surface_form
  - If normalized differs, Token NORMALIZED_TO normalized_form

Keep it minimal and queryable.
```

> Ran 

---

## S01-11 Add Lemma layer properly (Medium)

```
Add Lemma support as a first-class layer without pretending we can lemmatize yet.

Ingest change:
- Create a Lemma node for each normalized Form (temporary 1:1 mapping)
  lemma_id: "<language>:<lemma_headword>"
  headword: normalized orthography
  language: same as form
  pos: "UNKNOWN" for now
- Link (Form)-[:REALIZES]->(Lemma)

Add a TODO comment that real lemmatization will replace the 1:1 mapping.

Do not break rerunnability.
```

> Ran 

---

## S01-12 Add “Claim + Source” framework (High)

```
Implement Claim and Source nodes to represent uncertain scholarship.

Add:
- Node Source {source_id, citekey, title, year, authors?, url?}
- Node Claim {claim_id, type, statement, confidence, status}
- Edges:
  (Claim)-[:SUPPORTED_BY]->(Source)
  (Claim)-[:ASSERTS]->(Lemma)  (generic)
  (Claim)-[:ASSERTS]->(Etymon) (generic)
  (Claim)-[:CONTRADICTS]->(Claim)

Add repository methods to upsert Source and Claim and to link them.

Add a docs section explaining why claims exist (competing etymologies, disagreement).
No UI needed.
```

> Ran 

---

## S01-13 Model “Norway” as a worked example dataset (Medium)

```
Add a script scripts/seed_norway_example.py that inserts a worked example:

Create Lemmas:
- Old Norse: Nóregr
- Nynorsk: Noreg
- Bokmål: Norge
- English: Norway

Create Forms for each lemma (at least one each).
Add historical links:
- (nn_Noreg)-[:DERIVES_FROM]->(non_Noregr)
- (nb_Norge)-[:DERIVES_FROM]->(da_Norge) and (da_Norge)-[:DERIVES_FROM]->(non_Noregr) OR simplified direct link
- (en_Norway)-[:BORROWED_FROM]->(non_Noregr)

Add competing etymology Claims:
- Claim A: norð + vegr
- Claim B: nór + vegr
Each supported by a Source placeholder.

Ensure reruns do not duplicate nodes.

This script is for documentation/testing only.
```

> Ran 

---

## S01-14 Add alignment model (Extra High)

```
Add translation/alignment primitives.

Schema:
- (Edition)-[:TRANSLATES]->(Edition)
- (Segment)-[:ALIGNED_TO {method, confidence}]->(Segment)

Add a script scripts/align_demo.py:
- Creates a mock “translation edition” for a small subset of Havamal (e.g. first stanza)
- Creates segment-to-segment alignments with confidence=1.0 (manual)
- Leaves token-to-token translation links for later

Document intended future: token-level alignment derived from segment alignment.
```

> Ran 

---

## S1-02a Update ingest to this exact JSON structure (Medium)

```
Update scripts/ingest_havamal_json.py to parse the exact structure of data/Hávamál1.json:

JSON shape:
information.cover[] (strings)
information.writer[] (strings)
poem.title (string)
poem.verses[] each has:
  verse (e.g. "I.")
  strophes[] each has:
    strophe (e.g. "1.")
    lines[] (strings)

Graph:
- Work {work_id:"havamal"}
- Edition {edition_id:"havamal_gudni_jonsson_print"} (use ascii in id)
  props: title=poem.title, cover=array, writer=array, language="Old Norse"
- Segment: create ONE per line
  segment_id: f"{edition_id}:v{verse}:s{strophe}:l{line_index}"
  props: verse, strophe, line_index, ref (e.g. "I.1.0"), text (line string)
- Token: tokenize each line
  token_id: f"{segment_id}:t{token_index}"
  props: surface, normalized, position
- Form: one per (language + surface)
  form_id: f"non:{surface}"
  props: orthography=surface, language="Old Norse"
- Relationships:
  Work-[:HAS_EDITION]->Edition
  Edition-[:HAS_SEGMENT]->Segment
  Segment-[:HAS_TOKEN]->Token
  Token-[:INSTANCE_OF_FORM]->Form

Requirements:
- MERGE everywhere (rerunnable)
- Implement normalize(surface): strip surrounding punctuation + collapse whitespace; keep diacritics
- Tokenizer: split on whitespace, then strip punctuation from ends, skip empty tokens
- Do not add Lemma yet
- Add small sanity prints: number of segments and tokens ingested
- Do not modify JSON source file
```

> Ran 

---
